{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the data-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 23:14:43.983753 140435088435008 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-571b30c15455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprotos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprotos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work2/EK100/modeling/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprotos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work2/EK100/models/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprotos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_det\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFullyDet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweakly_det\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeaklyDet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_time\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleTimeDet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work2/EK100/models/fully_det.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprotos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_evaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectionEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_process\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmasked_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work2/EK100/models/detection_evaluation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluate_detection_json_ek100\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mANETdetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work2/EK100/C2-Action-Detection/EvaluationCode/evaluate_detection_json_ek100.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from modeling import trainer\n",
    "from protos import model_pb2\n",
    "from protos import pipeline_pb2\n",
    "from readers import reader\n",
    "\n",
    "\n",
    "model_dir = \"logs/baseline/\"\n",
    "video_lengths_csv = \"epic-kitchens-100-annotations/EPIC_100_video_info.csv\"\n",
    "verb_classes_csv = \"epic-kitchens-100-annotations/EPIC_100_verb_classes.csv\"\n",
    "noun_classes_csv = \"epic-kitchens-100-annotations/EPIC_100_noun_classes.csv\"\n",
    "\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "with tf.io.gfile.GFile(os.path.join(model_dir, 'pipeline.pbtxt'), 'r') as fp:\n",
    "  pipeline_proto = text_format.Merge(fp.read(), pipeline_pb2.Pipeline())\n",
    "\n",
    "# HERE WE VISUALIZE TRAIN SET PREDICTIONS.\n",
    "input_fn = reader.get_input_fn(pipeline_proto.train_reader, is_training=False)\n",
    "model_fn = trainer.create_model_fn(pipeline_proto)\n",
    "\n",
    "features, labels = input_fn().make_one_shot_iterator().get_next()\n",
    "predictions = model_fn(features, labels, tf.estimator.ModeKeys.PREDICT, None).predictions\n",
    "\n",
    "def data_generator():\n",
    "  saver = tf.train.Saver()\n",
    "  with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    while True:\n",
    "      yield sess.run([predictions, labels])\n",
    "\n",
    "dg = data_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the per-segment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from matplotlib import colors\n",
    "from scipy.special import softmax\n",
    "\n",
    "STANDARD_COLORS = [\n",
    "    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque', 'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n",
    "    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan', 'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n",
    "    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet', 'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n",
    "    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod', 'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n",
    "    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue', 'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n",
    "    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue', 'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n",
    "    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid', 'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n",
    "    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin', 'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n",
    "    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed', 'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n",
    "    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown', 'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n",
    "    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow', 'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat',\n",
    "    'WhiteSmoke', 'Yellow', 'YellowGreen'\n",
    "]\n",
    "random.seed(286)\n",
    "random.shuffle(STANDARD_COLORS)\n",
    "STANDARD_COLORS = ['White'] + STANDARD_COLORS\n",
    "\n",
    "\n",
    "def show_color_bar(data, classid2colorid, classid2name, title, trim_to=100, show_ticks=True):\n",
    "  observation_window = data.shape[0]\n",
    "  image = np.full((10, observation_window, 3), 255)\n",
    "  for i in range(observation_window):\n",
    "    color_id = classid2colorid[data[i]]\n",
    "    r, g, b = colors.to_rgb(STANDARD_COLORS[color_id].lower())\n",
    "    image[:, i, 0] = int(r * 255)\n",
    "    image[:, i, 1] = int(g * 255)\n",
    "    image[:, i, 2] = int(b * 255)\n",
    "    \n",
    "  plt.figure(figsize=(20, 2))\n",
    "  ax = plt.subplot(111)\n",
    "  ax.matshow(image[:, :trim_to, :])\n",
    "  if show_ticks:\n",
    "    ax.set_xticks(np.arange(trim_to))\n",
    "    ax.set_xticklabels([classid2name.get(x, '') + '({})'.format(x) for x in data[:trim_to]])\n",
    "  plt.title(title)\n",
    "  plt.xticks(rotation=90)\n",
    "\n",
    "    \n",
    "def load_id_to_name(file_name):\n",
    "  df = pd.read_csv(file_name)\n",
    "  return {i + 1: v for i, v in zip(df['id'], df['key'])}\n",
    "\n",
    "\n",
    "classid2verb = load_id_to_name(verb_classes_csv)\n",
    "classid2noun = load_id_to_name(noun_classes_csv)\n",
    "\n",
    "elem = next(dg)\n",
    "y_pred, y_true = elem\n",
    "y_pred_noun, y_pred_verb = y_pred['noun_logits'][0].argmax(-1), y_pred['verb_logits'][0].argmax(-1)\n",
    "y_true_noun, y_true_verb = y_true[0][0], y_true[1][0]\n",
    "observation_window = y_true_verb.shape[0]\n",
    "\n",
    "verbid2colorid = {c: i for i, c in enumerate(sorted(set(y_true_verb.tolist() + y_pred_verb.tolist())))}\n",
    "show_color_bar(y_true_verb, verbid2colorid, classid2verb, \"verb-true\")\n",
    "show_color_bar(y_pred_verb, verbid2colorid, classid2verb, \"verb-pred\")\n",
    "\n",
    "nounid2colorid = {c: i for i, c in enumerate(sorted(set(y_true_noun.tolist() + y_pred_noun.tolist())))}\n",
    "show_color_bar(y_true_noun, nounid2colorid, classid2noun, \"noun-true\")\n",
    "show_color_bar(y_pred_noun, nounid2colorid, classid2noun, \"noun-pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the confidense score of specific classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noun_scores, verb_scores = softmax(y_pred['noun_logits'][0], -1), softmax(y_pred['verb_logits'][0], -1)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.bar(np.arange(100), verb_scores[:100, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from models.post_process import py_post_process\n",
    "\n",
    "def load_video_lengths(file_name):\n",
    "  df = pd.read_csv(file_name, sep=',')\n",
    "  return {i: l for i, l in zip(df['video_id'], df['duration'])}\n",
    "\n",
    "trim_to = 100\n",
    "max_n_detection = 100\n",
    "video_lengths = load_video_lengths(video_lengths_csv)\n",
    "\n",
    "(i_starts, i_ends, verb_ids, confidences\n",
    " ) = py_post_process(verb_scores[:trim_to, 1:], \n",
    "                     max_n_detection=max_n_detection, \n",
    "                     score_min=0.1, \n",
    "                     score_max=0.1, \n",
    "                     score_step=0.2)\n",
    "\n",
    "show_color_bar(y_true_verb, verbid2colorid, classid2verb, \"verb-true\", show_ticks=False)\n",
    "\n",
    "images = []\n",
    "for i_start, i_end, verb_id, confidence in zip(i_starts, i_ends, verb_ids, confidences):\n",
    "  verb_id += 1\n",
    "  if not verb_id in verbid2colorid: continue\n",
    "        \n",
    "  color_id = verbid2colorid[verb_id]\n",
    "  image = np.full((1, trim_to, 3), 255)\n",
    "  r, g, b = colors.to_rgb(STANDARD_COLORS[color_id].lower())\n",
    "  for i in range(i_start, 1 + i_end):\n",
    "    image[:, i, 0] = int(r * 255)\n",
    "    image[:, i, 1] = int(g * 255)\n",
    "    image[:, i, 2] = int(b * 255)\n",
    "  images.append(image)\n",
    "\n",
    "image = np.concatenate(images, 0)\n",
    "plt.figure(figsize=(20, 200))\n",
    "ax = plt.subplot(111)\n",
    "ax.matshow(image[:, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
